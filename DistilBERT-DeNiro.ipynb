{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b701df88-b647-4a91-8194-adfe9405ae55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here we fine-tune the encoder-only DilstilBERT model, a model trained using a knowledge distillation process from BERT. The model is domain adapated (fine-tuned) on the IMDB dataset for an ultimate masked language model task that fills [MASK] tokens in sentences with an empsasis on terminology common to movies. We publish this model to the Hugging Face hub with the name [DistilBERT-DeNiro](https://huggingface.co/MarioBarbeque/DistilBERT-DeNiro/tree/main).\n",
    "\n",
    "Rather than using the simpler 🤗 `Trainer` API, we will build our training loop directly in PyTorch for more control over simple details. In the Azure Databricks notebook where we have completed previous model training ([RoBERTa-base-DReiFT](https://huggingface.co/MarioBarbeque/RoBERTa-base-DReiFT), for example), we found that configuring single-node multi-GPU distributed training with PyTorch's `DistributedDataParallel` and `DistributedSampler` classes was tediously tricky in the Databricks notebook environment. For models like RoBERTa-base-DReiFT, we instead made use of the simpler, self-contained 🤗 `Trainer` API and wrapped it in the PySpark `TorchDistributor` class which is designed to orchestrate distributed training of PyTorch models through Apache Spark.\n",
    "\n",
    "For the training of this model, we use a single-node single Nvidia T4 GPU compute instance to train our model locally without any distributed processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afcad229-ca17-449b-9b77-8ca4de65f453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preprocessing and Prep for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c2904f7-c142-487e-9e8a-c9c13aa28225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# evaluate GPU memory of this single node with an Nvidia T4 GPU\n",
    "import torch\n",
    "\n",
    "def mem_status(): \n",
    "    if torch.cuda.is_available():\n",
    "        gpus = torch.cuda.device_count()\n",
    "        print(\"Memory status: \")\n",
    "        for i in range(gpus):\n",
    "            properties = torch.cuda.get_device_properties(i)\n",
    "            total_memory = properties.total_memory / (1024 ** 3)  # Convert to GB\n",
    "            allocated_memory = torch.cuda.memory_allocated(i) / (1024 ** 3)  # Convert to GB\n",
    "            reserved_memory = torch.cuda.memory_reserved(i) / (1024 ** 3)  # Convert to GB\n",
    "            available_memory = total_memory - reserved_memory\n",
    "            print(f\"GPU {i}:\")\n",
    "            print(f\"  Total memory: {total_memory:.2f} GB\")\n",
    "            print(f\"  Allocated memory: {allocated_memory:.2f} GB\")\n",
    "            print(f\"  Reserved memory: {reserved_memory:.2f} GB\")\n",
    "            print(f\"  Available memory: {available_memory:.2f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available.\")\n",
    "\n",
    "# we'll make use of this gradually to keep track of our GPU utilization\n",
    "mem_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f74d18d-753a-4d56-b195-1c963bd31eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# first grab the DistilBERT model\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5029e925-c988-435d-a859-fdcb21d0db97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# grab the tokenizer as well\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7514d0-27f2-4651-9dd1-6ab42aad6bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's play around with DistilBERT's masked language model with an example before we fine-tune it on data related to movies\n",
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a157f707-c303-4558-9163-66c5517beca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# given the text and its tokenization, which are the top 5 results likely to fill the [MASK] token as predicted by the pre-trained model?\n",
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")\n",
    "\n",
    "# now lets fine-tune this model to give us more movie-specific responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c1c737-687a-4e4c-878b-c49248d0ce5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load the large movie reviews dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9636969b-b1d9-4e8e-8f85-8d67b32332e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# grab a random sample of the data\n",
    "sample = imdb_dataset[\"train\"].shuffle(seed=92).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
    "    print(f\"'>>> Label: {row['label']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aad1fe9-1cb9-4484-ab3f-b7a4aab3af6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for later pre-training tasks, we'll peek at the unsupervised split evident here in the dataset\n",
    "sample = imdb_dataset[\"unsupervised\"].shuffle(seed=92).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
    "    print(f\"'>>> Label: {row['label']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3997d71-85c5-4dbd-b110-0c4df08c61d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# first, tokenizer our whole dataset without truncating before we concatenate the whole thing and split it evenly\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# batched=True for multithreading, disperse of unnecessary label and text columns\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "793ebb0b-7b8f-4438-aa03-d707e84bb013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# before chunking our text, we want to check out the max token context window for our model\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f537404-c3dc-4a68-92da-8fbd819f99f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# lets given 256 a try 🤷‍♂️\n",
    "chunk_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af7c2c7-2baa-4734-8a18-fcdb60d16a75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# grab a small sample to design our chunk and concatenation method\n",
    "\n",
    "# slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49b3101-d629-4558-9f4a-28f546f43636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use dict comprehension to create a dict of our concatenated samples\n",
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10220d3f-a27f-4064-a90e-c234bce4f729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now chunk this concatenation based on our maximum chunk size into nested lists of maximum chunk size length\n",
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb539b00-49d8-4c8e-9aa9-750d400b823e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# with the last column of smaller length above, we can either drop it, or we can pad it to match the length of the others\n",
    "# we'll take the padding approach as follows\n",
    "\n",
    "# we'll pad to the input_ids col the tokenizer's pad_token_id, which we confirm with:\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id))\n",
    "\n",
    "# similarly, each chunk is tokenized individually so the attention_mask for all tokens is 1, we'll just pad this col with 1s\n",
    "\n",
    "# and lastly, in regards to word_ids col created by the tokenizer, the hugging face documentation states:\n",
    "# 'A list indicating the word corresponding to each token. Special tokens added by the tokenizer are mapped to None and other tokens are mapped to the index of their corresponding word (several tokens will be mapped to the same word index if they are parts of that word).'\n",
    "# so we will just tack on a bunch of Nones to match the max_chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f502a72b-f339-41c0-a13b-4630326318f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we'll take the padding approach as follows, defining a function to be used in our preprocessing\n",
    "def pad_last_chunk(examples, chunk_size):\n",
    "  while len(examples[\"input_ids\"][-1]) < chunk_size:\n",
    "    examples[\"input_ids\"][-1].append(tokenizer.pad_token_id)\n",
    "  while len(examples[\"attention_mask\"][-1]) < chunk_size:\n",
    "    examples[\"attention_mask\"][-1].append(1)\n",
    "  while len(examples[\"word_ids\"][-1]) < chunk_size:\n",
    "    examples[\"word_ids\"][-1].append(None)\n",
    "\n",
    "pad_last_chunk(chunks, chunk_size)\n",
    "\n",
    "# double check out lengths\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> input_ids chunk length: {len(chunk)}'\")\n",
    "for chunk in chunks[\"attention_mask\"]:\n",
    "    print(f\"'>>> attention_mask chunk length: {len(chunk)}'\")\n",
    "for chunk in chunks[\"word_ids\"]:\n",
    "    print(f\"'>>> word_ids chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e8cd67-ead1-4393-a815-a99f43d8a7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# nice, now lets define our function for chunking, concatenating, and padding the whole dataset\n",
    "def group_texts(examples):\n",
    "    # concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # pad last column to match chunk_size\n",
    "    pad_last_chunk(result, chunk_size)\n",
    "    # create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9efed0f1-b173-4246-9caf-a294a50b3612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now we apply this function by mapping it to our dataset\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fffc7695-fd67-49d7-b5d1-8d36dc01ce4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# its worth noting the following:\n",
    "assert tokenized_datasets[\"train\"].num_rows < lm_datasets[\"train\"].num_rows\n",
    "# which shows that upon chunking and concatenating the reviews, we now have examples involving 'contiguous tokens' that span across multiple examples from the original corpus. As such, our chunked and concatenated row count of labels is now larger than the row count of the original dataset of reviews. This is evident as indicated by the presence of special tokens like [CLS] [SEP] and [PAD]. \n",
    "\n",
    "# We can see this as follows:\n",
    "print(tokenizer.decode(lm_datasets[\"train\"][2313][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bcb8eeb-1e5a-414f-b3ad-a8286617bc4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# before we train our model with a custom PyTorch training loop, we address the 'perplexity' metric used to evaluate our results\n",
    "# https://en.wikipedia.org/wiki/Cross-entropy\n",
    "# https://en.wikipedia.org/wiki/Perplexity\n",
    "\n",
    "# mathematically, we define perplexity to be the exponential of our cross-entropy loss (or as the wiki states, 2**cross-loss entropy)\n",
    "# intuitively, this tells us how \"suprised\" or \"perplexed\" the model is when seeing input tokens -  the lower the perplexity, the better our model is at predicting the next token\n",
    "\n",
    "# we'll use a specific example from the gpt-2 model to demonstrate\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "example_input = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokenized_example = gpt_tokenizer(example_input, return_tensors=\"pt\")\n",
    "# by passing the labels as the inputs themselves, the loss corresponds to the cross entropy between the input and output sequences\n",
    "example_cross_entropy_loss = gpt_model(input_ids=tokenized_example[\"input_ids\"], labels=tokenized_example[\"input_ids\"]).loss\n",
    "\n",
    "# compute the various forms of plexity for this example\n",
    "print(f\">>> Perplexity e^H(p,q): {math.exp(example_cross_entropy_loss):.2f}\")\n",
    "print(f\">>> Perplexity 2^H(p,q): {2**(example_cross_entropy_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe218a28-b1f8-4925-ad30-6a56aa3afbed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# verify we've used only the CPU so far before ultimately moving our model to the GPU for training\n",
    "mem_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee955e6-f504-49da-9d5e-de7cd28515c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now we define our data collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec54070-847d-4f80-aed1-e8c70fea9cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we can see how this datacollator works by comparing some examples\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9163a30a-60ba-4a8e-b1f3-ee149c002fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# while the above example shows indeed that our data collator is randomly masking tokens, we note that the default 'DataCollatorForLanguageModeling' does not allow for whole word masking - by default it masks just tokens, potentially masking in the middle of a word\n",
    "# we construct a whole word masking function instead\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.15\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f261ad-7db8-405f-acaf-324676744a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# double check our function on the same samples as before\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f95caf-3c9a-4754-a7ed-6458cf060842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# before running our evaluation, we want to apply a single masking to the evaluation dataset\n",
    "# if we let the masking be applied with each iteration of the training loop, there will be variation in how our evaluation dataset is masked each time, introducing some randomness in results perplexity scores\n",
    "# to avoid this, we apply the masking a single time before training and iterative evaluation each epoch\n",
    "\n",
    "# lastly, we will opt for the whole word masking approach and make use of our previous function\n",
    "\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = whole_word_masking_data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add4f656-d3db-458b-a0f5-c9ebf7155d19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we map this single masking onto the whole test dataset\n",
    "eval_dataset = lm_datasets[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=lm_datasets[\"test\"].column_names, # remove old columns now prefixed with masked_\n",
    ")\n",
    "# rename columns to match other splits\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ed58a3-52dc-459b-b60f-04678c501292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now we configure our dataloaders before pumping the data into the model for training and evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# previously OOMd with batch size 64\n",
    "batch_size = 32\n",
    "\n",
    "# for the train split we pass the custom whole word masking collator\n",
    "train_dataloader = DataLoader(\n",
    "    lm_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=whole_word_masking_data_collator,\n",
    ")\n",
    "# since we premasked our evaluation dataset to remove randomness, we simply use the default data collator\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9019ebb6-d39d-4202-b9d2-10862ccf51cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742dddd3-41f7-4c34-ae76-4470f7935737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we now load a fresh version of the model onto the GPU and check the mem status\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint).to(torch.device(\"cuda\"))\n",
    "mem_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a4f966c-84ab-4e6b-90ff-423bb3b810a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now we define our optimizer as AdamW\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55aef0f-3d51-4b3d-8065-5b0ffe4fc6f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prepare everything with the 🤗 accelerate package\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a598bfb0-018a-4287-89f9-db9ecf46f50e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# configure a learning rate scheduler\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "num_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97966f92-4593-42bc-ba83-6ea3626e5fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check our memory status before we begin our loop -  we iteratively check it each epoch as training as well\n",
    "mem_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28cf9725-2976-4b2e-962c-d12d4a3433f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now we run our training loop!\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "output_dir = \"/Volumes/workspace_dogfood/jgr/hugging_face_cache/DistilBERT-DeNiro\"\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        # accelerate package handles this inherently, but I prefer to see it directly for future clarity\n",
    "        batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
    "        # batch = {k: v.to(torch.device(\"cuda\")) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        # loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        # same note as above\n",
    "        batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
    "        # batch = {k: v.to(torch.device(\"cuda\")) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "    mem_status()\n",
    "\n",
    "    # save and upload\n",
    "    # first wait for all processes to reach the same stage\n",
    "    accelerator.wait_for_everyone()\n",
    "    # unwraps the model from accelerate.prepare() to reintroduce the save_pretrained() fn for saving\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    # accelerator.save() instead of torch.save()\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        # unable to use this currently since we cannot clone this repo into our Databricks workspace\n",
    "        # repo.push_to_hub(\n",
    "        #     commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68ec84ff-a806-4139-aac8-a3be1f2da24d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Post-training Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be93ac4b-e30e-4726-b57c-a26228cd1b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mem_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c150fa-24ea-43d8-80ac-4bb000adf55e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we ran out of memory when training the model with a batch size of 64\n",
    "# we derive a rough estimate of the memory required for training a model as a function of its config\n",
    "\n",
    "\"\"\"\n",
    "A function for giving a rough estimate on the amount of memory required to train a model without activation checkpointing.\n",
    "\n",
    "l: number of layers\n",
    "p: precision\n",
    "s: sequence length\n",
    "b: batch size\n",
    "h: hidden size\n",
    "a: number of attention heads \n",
    " \"\"\"\n",
    "def mem_required(l, p, s, b, h, a):\n",
    "  total_bytes = l*p*s*b*h*(16+(2/p)+(2*a*s/h)+a*s/(p*h))\n",
    "  return f\"{total_bytes/(1024**3):.2f} GB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60df0498-c861-4175-803a-ef6f1a3d9b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# compute the mem required for training DistilBERT with our initial vs final choices\n",
    "print(\"batch size 64 required ~ \" + mem_required(6, 4, 256, 64, 3072, 12))\n",
    "print(\"batch size 32 required ~ \" + mem_required(6, 4, 256, 32, 3072, 12))\n",
    "\n",
    "# this checks out - we only have a 16GB machine to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6d8592b-536d-493f-b9aa-3ac7b759709c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# comparing our domain-adapted DistilBERT-DeNiro to the standard DistilBERT after fine-tuning\n",
    "# previously the standard DistilBERT model gave the following output on this text prompt in cell 6 above:\n",
    "\"\"\"\n",
    "'>>> This is a great deal.'\n",
    "'>>> This is a great success.'\n",
    "'>>> This is a great adventure.'\n",
    "'>>> This is a great idea.'\n",
    "'>>> This is a great feat.'\n",
    "\n",
    "\"\"\"\n",
    "# let's see how our newly trained model behaves!\n",
    "\n",
    "text = \"This is a great [MASK].\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "\n",
    "# find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c319eb12-e864-475b-a9b0-071a1da2e0b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's see how the fill mask works on a specific example with a little prompt engineering\n",
    "text_2 = \"[MASK] Tarantino is a really creative director!\"\n",
    "\n",
    "inputs_2 = tokenizer(text_2, return_tensors=\"pt\")\n",
    "token_logits_2 = model(**inputs_2).logits\n",
    "mask_token_index_2 = torch.where(inputs_2[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits_2 = token_logits_2[0, mask_token_index_2, :]\n",
    "\n",
    "fill = torch.topk(mask_token_logits_2, 1, dim=1).indices[0].tolist()\n",
    "print(text_2.replace(tokenizer.mask_token, tokenizer.decode([fill[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6de2b51d-dd55-46a1-b603-9f305336de8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# looks like our model is still wrapped in a (HF? not PyTorch?) DDP by hugging face accelerate\n",
    "model\n",
    "# this prevents us from pushing it to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6396849a-aeea-447e-b0b0-bce62b7e63fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's unwrap it\n",
    "unwrapped_model = accelerator.unwrap_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e97e68f-b232-4f89-a15e-61953af47df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now sign into the hub and push our model\n",
    "dbutils.widgets.text(\"hf_token\", \"\", \"hf_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6029541a-e8d5-4d25-afa5-176915e41742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hf_token = dbutils.widgets.get(\"hf_token\")\n",
    "!huggingface-cli login --token $hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4773156b-73d2-4bea-b094-e3ba0f2c7877",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unwrapped_model.push_to_hub(\"DistilBERT-DeNiro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c6f9db-65b1-4efa-b37f-78256a7e6a4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we could also have grabbed our model from the saved location for pushing like so:\n",
    "trained_model = AutoModelForMaskedLM.from_pretrained(\"/Volumes/workspace_dogfood/jgr/hugging_face_cache/DistilBERT-DeNiro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65af16a6-5369-4afb-a5cf-334fd3b8b808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# detail how end users can use this model - for the model card\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"MarioBarbeque/DistilBERT-DeNiro\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "# Pass a unique string with a [MASK] token for the model to fill\n",
    "text = \"This is a great [MASK]!\"\n",
    "\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "token_logits = model(**tokenized_text).logits\n",
    "\n",
    "mask_token_index = torch.where(tokenized_text[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "for token in top_5_tokens:\n",
    "  print(text.replace(tokenizer.mask_token, tokenizer.decode(token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0905917-35e1-4615-b3de-783433856d9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Fine-tuning DistilBERT for a MLM task",
   "widgets": {
    "hf_token": {
     "currentValue": "",
     "nuid": "ed6858fc-de58-4cb4-b6f9-76a186bd7f01",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "hf_token",
      "name": "hf_token",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "hf_token",
      "name": "hf_token",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
